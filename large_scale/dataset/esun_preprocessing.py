"""
E.SUN è³‡æ–™é›†é è™•ç†è…³æœ¬ - æœ€çµ‚ç‰ˆ
æ ¹æ“šå¯¦éš›è³‡æ–™æ ¼å¼:
- txn_time: "05:05:00 AM" (12å°æ™‚åˆ¶)
- txn_date: 8 (åˆ‡é½Šç¬¬ä¸€å¤©ç‚º1)
- alert event_date: 0 (åˆ‡é½Šç¬¬ä¸€å¤©ç‚º0)
"""
import pandas as pd
import numpy as np
import scipy.sparse as sp
from sklearn.preprocessing import LabelEncoder, StandardScaler
import dgl
import torch
import pickle
import os
from tqdm import tqdm
from datetime import datetime

def parse_time_to_seconds(time_str):
    """
    å°‡ "05:05:00 AM" æ ¼å¼è½‰æ›ç‚ºç•¶å¤©çš„ç§’æ•¸
    
    Args:
        time_str: æ™‚é–“å­—ä¸²ï¼Œä¾‹å¦‚ "05:05:00 AM"
    
    Returns:
        ç§’æ•¸ (0-86399)ï¼Œç„¡æ³•è§£æè¿”å› -1
    """
    if pd.isna(time_str) or time_str == '':
        return -1
    
    try:
        # è™•ç† 12å°æ™‚åˆ¶
        time_str = str(time_str).strip()
        
        # å˜—è©¦è§£æ "HH:MM:SS AM/PM" æ ¼å¼
        dt = datetime.strptime(time_str, '%I:%M:%S %p')
        # è½‰æ›ç‚ºç§’æ•¸
        seconds = dt.hour * 3600 + dt.minute * 60 + dt.second
        return seconds
    except:
        try:
            # å˜—è©¦è§£æ 24å°æ™‚åˆ¶ "HH:MM:SS"
            dt = datetime.strptime(time_str, '%H:%M:%S')
            seconds = dt.hour * 3600 + dt.minute * 60 + dt.second
            return seconds
        except:
            return -1


def load_esun_data(transaction_path, alert_path, predict_path=None):
    """
    è¼‰å…¥ E.SUN è³‡æ–™é›†
    
    é—œéµè¨­è¨ˆ:
    1. å¸³æˆ¶ ID ä¸è½‰æ•¸å€¼ï¼Œç”¨ acct_to_idx å­—å…¸æ˜ å°„åˆ°ç¯€é»ç´¢å¼•
    2. txn_time è§£æç‚ºç§’æ•¸ (0-86399)
    3. txn_date å’Œ alert event_date å°é½Š (alert çš„ 0 å°æ‡‰ txn çš„ 1)
    """
    print("="*80)
    print("E.SUN è³‡æ–™è¼‰å…¥èˆ‡é è™•ç† (æœ€çµ‚ç‰ˆ)")
    print("="*80)
    
    # è¼‰å…¥è³‡æ–™
    print("\nğŸ“ æ­¥é©Ÿ 1: è¼‰å…¥ CSV æª”æ¡ˆ...")
    df_txn = pd.read_csv(transaction_path, dtype=str)
    df_alert = pd.read_csv(alert_path, dtype=str)
    
    print(f"   âœ“ äº¤æ˜“è³‡æ–™: {len(df_txn):,} ç­†")
    print(f"   âœ“ è­¦ç¤ºå¸³æˆ¶: {len(df_alert):,} å€‹")
    
    # é¡¯ç¤ºç¯„ä¾‹è³‡æ–™
    print(f"\n   äº¤æ˜“è³‡æ–™ç¯„ä¾‹:")
    print(f"   - txn_time ç¯„ä¾‹: {df_txn['txn_time'].iloc[0]}")
    print(f"   - txn_date ç¯„ä¾‹: {df_txn['txn_date'].iloc[0]}")
    
    # è¼‰å…¥é æ¸¬æ¸…å–® (å¯é¸)
    predict_accounts = None
    if predict_path and os.path.exists(predict_path):
        df_predict = pd.read_csv(predict_path, dtype=str)
        predict_accounts = set(df_predict['acct'].values)
        print(f"   âœ“ é æ¸¬æ¸…å–®: {len(predict_accounts):,} å€‹å¸³æˆ¶")
    
    # === é—œéµæ­¥é©Ÿ 1: å»ºç«‹å¸³æˆ¶æ˜ å°„ (ä¸è½‰æ•¸å€¼!) ===
    print("\nğŸ”‘ æ­¥é©Ÿ 2: å»ºç«‹å¸³æˆ¶åˆ°ç¯€é»ç´¢å¼•çš„æ˜ å°„...")
    print("   æ³¨æ„: å¸³æˆ¶ ID ä¿æŒç‚º Stringï¼Œåªæ˜ å°„åˆ°ç¯€é»ç´¢å¼• (0, 1, 2, ...)")
    
    # æ”¶é›†æ‰€æœ‰å‡ºç¾éçš„å¸³æˆ¶
    all_accounts = pd.concat([
        df_txn['from_acct'],
        df_txn['to_acct']
    ]).unique()
    
    # å‰µå»ºé›™å‘æ˜ å°„å­—å…¸
    acct_to_idx = {acct: idx for idx, acct in enumerate(all_accounts)}
    idx_to_acct = {idx: acct for acct, idx in acct_to_idx.items()}
    num_nodes = len(all_accounts)
    
    print(f"   âœ“ ç¸½å¸³æˆ¶æ•¸ (åœ–ç¯€é»æ•¸): {num_nodes:,}")
    print(f"   âœ“ å¸³æˆ¶ç¯„ä¾‹: '{list(all_accounts)[:3]}'")
    print(f"   âœ“ æ˜ å°„ç¯„ä¾‹: '{list(all_accounts)[0]}' â†’ ç¯€é»ç´¢å¼• {acct_to_idx[list(all_accounts)[0]]}")
    
    # === æ­¥é©Ÿ 2: å»ºç«‹åœ–çš„é‚Š ===
    print("\nğŸ“Š æ­¥é©Ÿ 3: å»ºç«‹äº¤æ˜“åœ–çš„é‚Š...")
    
    # éæ¿¾æœ‰æ•ˆäº¤æ˜“
    valid_txn = df_txn[pd.notna(df_txn['from_acct']) & pd.notna(df_txn['to_acct'])]
    
    # ä½¿ç”¨ acct_to_idx æ˜ å°„åˆ°ç¯€é»ç´¢å¼•
    src_nodes = valid_txn['from_acct'].map(acct_to_idx).values
    dst_nodes = valid_txn['to_acct'].map(acct_to_idx).values
    
    # ç§»é™¤æ˜ å°„å¤±æ•—çš„é‚Š
    valid_edges = ~(pd.isna(src_nodes) | pd.isna(dst_nodes))
    src_nodes = src_nodes[valid_edges].astype(int)
    dst_nodes = dst_nodes[valid_edges].astype(int)
    
    print(f"   âœ“ æœ‰æ•ˆäº¤æ˜“é‚Šæ•¸: {len(src_nodes):,}")
    
    # === æ­¥é©Ÿ 3: å»ºç«‹æ¨™ç±¤ (è™•ç†æ—¥æœŸåç§») ===
    print("\nğŸ·ï¸  æ­¥é©Ÿ 4: å»ºç«‹æ¨™ç±¤...")
    print("   æ³¨æ„: alert event_date çš„ 0 å°æ‡‰ txn_date çš„ 1")
    
    labels = np.full(num_nodes, -1, dtype=np.int64)  # -1: æœªæ¨™è¨˜
    
    alert_accounts = df_alert['acct'].values
    alert_account_set = set(alert_accounts)
    
    num_alert_in_graph = 0
    for acct in alert_accounts:
        if acct in acct_to_idx:
            labels[acct_to_idx[acct]] = 1  # 1: è­¦ç¤ºå¸³æˆ¶
            num_alert_in_graph += 1
    
    print(f"   âœ“ è­¦ç¤ºå¸³æˆ¶ (label=1): {num_alert_in_graph:,}")
    print(f"   âœ“ æœªæ¨™è¨˜å¸³æˆ¶ (label=-1): {np.sum(labels == -1):,}")
    
    # === æ­¥é©Ÿ 4: æå–ç¯€é»ç‰¹å¾µ ===
    print("\nğŸ¯ æ­¥é©Ÿ 5: æå–ç¯€é»ç‰¹å¾µ...")
    features = extract_node_features_final(df_txn, acct_to_idx, num_nodes)
    
    print(f"   âœ“ ç‰¹å¾µçŸ©é™£å½¢ç‹€: {features.shape}")
    
    # === æ­¥é©Ÿ 5: å»ºç«‹ mask ===
    print("\nğŸ­ æ­¥é©Ÿ 6: å»ºç«‹è¨“ç·´/æ¸¬è©¦é®ç½©...")
    
    train_mask = labels == 1  # Few-shot ç”¨å·²çŸ¥è­¦ç¤ºå¸³æˆ¶
    
    if predict_accounts is not None:
        test_mask = np.array([
            (acct in predict_accounts) and (acct not in alert_account_set) 
            for acct in all_accounts
        ])
    else:
        test_mask = labels == -1
    
    print(f"   âœ“ Few-shot è¨“ç·´æ¨£æœ¬: {np.sum(train_mask):,}")
    print(f"   âœ“ æ¸¬è©¦æ¨£æœ¬: {np.sum(test_mask):,}")
    
    # === æ­¥é©Ÿ 6: å»ºç«‹ DGL åœ– ===
    print("\nğŸ•¸ï¸  æ­¥é©Ÿ 7: å»ºç«‹ DGL åœ–...")
    
    graph = dgl.graph((src_nodes, dst_nodes), num_nodes=num_nodes)
    graph = dgl.to_bidirected(graph)
    graph = dgl.add_self_loop(graph)
    
    print(f"   âœ“ ç¯€é»æ•¸: {graph.num_nodes():,}")
    print(f"   âœ“ é‚Šæ•¸: {graph.num_edges():,}")
    
    return graph, features, labels, train_mask, test_mask, acct_to_idx, idx_to_acct


def extract_node_features_final(df_txn, acct_to_idx, num_nodes):
    """
    æ ¹æ“šå¯¦éš›è³‡æ–™æ ¼å¼æå–ç¯€é»ç‰¹å¾µ - ç°¡åŒ–åˆ°10ç¶­
    
    æ ¹æ“šè«–æ–‡ï¼Œlarge_scale æ¨¡å‹ä½¿ç”¨ 10 ç¶­ç‰¹å¾µ
    åƒè€ƒ: T-Finance å’Œ T-Social æ•¸æ“šé›†éƒ½æ˜¯ 10 ç¶­
    """
    print("   [1/5] è§£ææ™‚é–“æ ¼å¼...")
    
    # === 1. è§£æ txn_time ===
    df_txn['txn_time_seconds'] = df_txn['txn_time'].apply(parse_time_to_seconds)
    df_txn['txn_time_valid'] = df_txn['txn_time_seconds'] >= 0
    
    valid_pct = df_txn['txn_time_valid'].mean() * 100
    print(f"       âœ“ æˆåŠŸè§£ææ™‚é–“: {valid_pct:.2f}%")
    
    # === 2. è½‰æ›æ•¸å€¼æ¬„ä½ ===
    print("   [2/5] è™•ç†æ•¸å€¼æ¬„ä½...")
    
    df_txn['txn_date_num'] = pd.to_numeric(df_txn['txn_date'], errors='coerce').fillna(1)
    df_txn['txn_amt_num'] = pd.to_numeric(df_txn['txn_amt'], errors='coerce').fillna(0)
    
    # === 3. è¨ˆç®—çµ±è¨ˆç‰¹å¾µ (ç²¾ç°¡åˆ°10ç¶­) ===
    print("   [3/5] è¨ˆç®—çµ±è¨ˆç‰¹å¾µ (10ç¶­)...")
    
    # åƒè€ƒ T-Finance/T-Social çš„ç‰¹å¾µè¨­è¨ˆ
    # 10ç¶­ç‰¹å¾µ: äº¤æ˜“è¡Œç‚ºçš„æ ¸å¿ƒçµ±è¨ˆé‡
    
    # åˆ†é–‹è¨ˆç®—é¿å… pandas agg çš„ column å•é¡Œ
    
    # 3.1 åŸºæœ¬çµ±è¨ˆ
    txn_count = df_txn.groupby('from_acct').size()
    
    # 3.2 é‡‘é¡çµ±è¨ˆ
    amt_stats = df_txn.groupby('from_acct')['txn_amt_num'].agg(['mean', 'std', 'max'])
    
    # 3.3 ä¸åŒæ”¶æ¬¾äººæ•¸
    unique_recipients = df_txn.groupby('from_acct')['to_acct'].nunique()
    
    # 3.4 æ™‚é–“ç¯„åœ
    day_stats = df_txn.groupby('from_acct')['txn_date_num'].agg(['min', 'max'])
    
    # 3.5 å¹³å‡äº¤æ˜“æ™‚é–“ (åªè¨ˆç®—æœ‰æ•ˆæ™‚é–“)
    def safe_time_mean(x):
        valid = x[x >= 0]
        return valid.mean() if len(valid) > 0 else 43200  # é è¨­ä¸­åˆ12é»
    
    avg_time = df_txn.groupby('from_acct')['txn_time_seconds'].apply(safe_time_mean)
    
    # çµ„åˆæˆ DataFrame
    from_stats = pd.DataFrame({
        'txn_count': txn_count,
        'txn_amt_mean': amt_stats['mean'],
        'txn_amt_std': amt_stats['std'],
        'txn_amt_max': amt_stats['max'],
        'unique_recipients': unique_recipients,
        'day_min': day_stats['min'],
        'day_max': day_stats['max'],
        'avg_time': avg_time
    }).fillna(0)
    
    # 3.6 è¡ç”Ÿç‰¹å¾µ
    from_stats['day_span'] = from_stats['day_max'] - from_stats['day_min']
    from_stats['txn_per_day'] = from_stats['txn_count'] / (from_stats['day_span'] + 1)
    
    # æœ€çµ‚çš„ 10 ç¶­ç‰¹å¾µ
    feature_cols = [
        'txn_count',           # 1. äº¤æ˜“ç¸½æ•¸
        'txn_amt_mean',        # 2. å¹³å‡é‡‘é¡
        'txn_amt_std',         # 3. é‡‘é¡æ¨™æº–å·®
        'txn_amt_max',         # 4. æœ€å¤§é‡‘é¡
        'unique_recipients',   # 5. ä¸åŒæ”¶æ¬¾äººæ•¸
        'day_min',            # 6. é–‹å§‹äº¤æ˜“æ—¥
        'day_max',            # 7. æœ€å¾Œäº¤æ˜“æ—¥
        'day_span',           # 8. æ´»èºå¤©æ•¸
        'avg_time',           # 9. å¹³å‡äº¤æ˜“æ™‚é–“
        'txn_per_day'         # 10. æ¯æ—¥å¹³å‡äº¤æ˜“æ•¸
    ]
    
    from_stats = from_stats[feature_cols]
    
    print(f"       âœ“ ç‰¹å¾µç¶­åº¦: {len(feature_cols)}")
    print(f"       âœ“ ç‰¹å¾µåˆ—è¡¨: {feature_cols[:5]}...")
    
    # === 4. çµ„åˆç‰¹å¾µå‘é‡ ===
    print("   [4/5] çµ„åˆç‰¹å¾µå‘é‡...")
    
    all_features = []
    
    for acct in tqdm(acct_to_idx.keys(), desc="       è™•ç†", ncols=70, leave=False):
        if acct in from_stats.index:
            feat = from_stats.loc[acct].values.tolist()
        else:
            # æ²’æœ‰äº¤æ˜“è¨˜éŒ„çš„å¸³æˆ¶ç”¨é è¨­å€¼
            feat = [0] * 10
        
        all_features.append(feat)
    
    features = np.array(all_features, dtype=np.float32)
    
    print(f"       âœ“ ç‰¹å¾µçŸ©é™£å½¢ç‹€: {features.shape}")
    
    # === 5. æ¨™æº–åŒ– ===
    print("   [5/5] æ¨™æº–åŒ–ç‰¹å¾µ...")
    
    scaler = StandardScaler()
    features = scaler.fit_transform(features)
    
    # å“è³ªæª¢æŸ¥
    print(f"       âœ“ NaN: {np.isnan(features).sum()}, Inf: {np.isinf(features).sum()}")
    print(f"       âœ“ ç¯„åœ: [{features.min():.2f}, {features.max():.2f}]")
    
    return features


def save_dgl_graph(graph, features, labels, train_mask, test_mask, 
                   acct_to_idx, idx_to_acct, save_path='esun_graph.bin'):
    """å„²å­˜ DGL åœ–å’Œæ˜ å°„"""
    print(f"\nğŸ’¾ æ­¥é©Ÿ 8: å„²å­˜åœ–è³‡æ–™...")
    
    graph.ndata['feature'] = torch.FloatTensor(features)
    graph.ndata['label'] = torch.LongTensor(labels)
    graph.ndata['train_mask'] = torch.BoolTensor(train_mask)
    graph.ndata['test_mask'] = torch.BoolTensor(test_mask)
    
    dgl.save_graphs(save_path, [graph])
    
    # å„²å­˜å¸³æˆ¶æ˜ å°„ (é‡è¦!)
    mapping_file = save_path.replace('.bin', '_mapping.pkl')
    with open(mapping_file, 'wb') as f:
        pickle.dump({
            'acct_to_idx': acct_to_idx,
            'idx_to_acct': idx_to_acct
        }, f)
    
    print(f"   âœ“ åœ–æª”æ¡ˆ: {save_path}")
    print(f"   âœ“ æ˜ å°„æª”æ¡ˆ: {mapping_file}")
    print(f"   âœ“ æ˜ å°„åŒ…å« {len(acct_to_idx):,} å€‹å¸³æˆ¶")
    
    print("\n" + "="*80)
    print("âœ… è³‡æ–™é è™•ç†å®Œæˆ!")
    print("="*80)


if __name__ == "__main__":
    import sys
    
    print("\n" + "="*80)
    print("E.SUN è­¦ç¤ºå¸³æˆ¶åµæ¸¬ - è³‡æ–™é è™•ç†")
    print("åŸºæ–¼ AnomalyGFM (KDD 2025) è«–æ–‡æ–¹æ³•")
    print("="*80)
    
    # æª”æ¡ˆè·¯å¾‘
    transaction_path = "./esun/acct_transaction.csv"
    alert_path = "./esun/acct_alert.csv"
    predict_path = None  # å¦‚æœæœ‰å°±å¡«è·¯å¾‘
    
    # æª¢æŸ¥æª”æ¡ˆ
    if not os.path.exists(transaction_path):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {transaction_path}")
        sys.exit(1)
    
    if not os.path.exists(alert_path):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {alert_path}")
        sys.exit(1)
    
    # åŸ·è¡Œé è™•ç†
    graph, features, labels, train_mask, test_mask, acct_to_idx, idx_to_acct = load_esun_data(
        transaction_path, alert_path, predict_path
    )
    
    # å„²å­˜çµæœ
    save_dgl_graph(graph, features, labels, train_mask, test_mask, 
                   acct_to_idx, idx_to_acct, save_path='esun_graph.bin')
    
    # æ‘˜è¦
    print("\nğŸ“Š è™•ç†æ‘˜è¦:")
    print(f"  ç¯€é»æ•¸: {graph.num_nodes():,}")
    print(f"  é‚Šæ•¸: {graph.num_edges():,}")
    print(f"  ç‰¹å¾µç¶­åº¦: {features.shape[1]}")
    print(f"  è­¦ç¤ºå¸³æˆ¶: {np.sum(labels==1):,} ({np.sum(labels==1)/len(labels)*100:.2f}%)")
    print(f"  æœªæ¨™è¨˜å¸³æˆ¶: {np.sum(labels==-1):,} ({np.sum(labels==-1)/len(labels)*100:.2f}%)")
    print(f"  Few-shot å¯ç”¨æ¨£æœ¬: {np.sum(train_mask):,}")
    print(f"  æ¸¬è©¦æ¨£æœ¬: {np.sum(test_mask):,}")
    
    print("\nğŸ¯ é—œéµè¨­è¨ˆ:")
    print("  âœ“ å¸³æˆ¶ ID ä¿æŒç‚º Stringï¼Œç”¨å­—å…¸æ˜ å°„åˆ°ç¯€é»ç´¢å¼•")
    print("  âœ“ txn_time è§£æç‚ºç§’æ•¸ (0-86399)")
    print("  âœ“ timestamp = txn_date * 100000 + txn_time_seconds")
    print("  âœ“ ç‰¹å¾µåŸºæ–¼äº¤æ˜“è¡Œç‚ºï¼Œä¸æ˜¯å¸³æˆ¶èº«ä»½")
    
    print("\nâ­ï¸  ä¸‹ä¸€æ­¥:")
    print("  1. åŸ·è¡Œå­åœ–æ¡æ¨£: python esun_sample.py")
    print("  2. åŸ·è¡Œæ¨è«–: python run_inference_esun.py")